{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "NOTE this was converted from an RMD file and needs to be edited to be a proper Jupyter notebook\n",
        "\n",
        "# Overdetermined linear systems\n",
        "\n",
        "**Learning objectives:**\n",
        "\n",
        "- Learn to \"solve\" overdetermined linear systems: $\\mathbf{A}\\mathbf{X} = \\mathbf{b}$ where  $\\mathbf{A}$ is $m \\times n$ and $m > n$. \n",
        "\n",
        "- Learn about QR factorization and apply it to overdetermined systems.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "using FundamentalsNumericalComputation;"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Fitting Functions to Data\n",
        "\n",
        "Example: \n",
        "\n",
        "```{julia}\n",
        "year = 1955:5:2000;\n",
        "temp = [ -0.0480, -0.0180, -0.0360, -0.0120, -0.0040,\n",
        "       0.1180, 0.2100, 0.3320, 0.3340, 0.4560 ];\n",
        "    \n",
        "scatter(year,temp,label=\"data\",\n",
        "    xlabel=\"year\",ylabel=\"anomaly (degrees C)\",leg=:bottomright)\n",
        "```\n",
        "\n",
        "- A polynomial interpolant would overfit the data:\n",
        "\n",
        "```{julia}\n",
        "t = @. (year-1950)/10;\n",
        "n = length(t);\n",
        "V = [ t[i]^j for i in 1:n, j in 0:n-1 ];\n",
        "c = V\\temp;\n",
        "\n",
        "p = Polynomial(c);\n",
        "f = yr -> p((yr-1950)/10);\n",
        "plot!(f,1955,2000,label=\"interpolant\")\n",
        "```\n",
        "\n",
        "- Instead approximate with lower degree polynomial:\n",
        "\n",
        "$$\n",
        "y \\approx f(t) = c_1 + c_2t + \\cdots + c_{n-1} t^{n-2} + c_n t^{n-1},\n",
        "$$\n",
        "Or as matrix multiplication:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix} y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_m \\end{bmatrix} \\approx\n",
        "\\begin{bmatrix}\n",
        "f(t_1)                               \\\\\n",
        "f(t_2)                               \\\\\n",
        "f(t_3)                               \\\\\n",
        "\\vdots                               \\\\\n",
        "f(t_m)\n",
        "\\end{bmatrix} =\n",
        "\\begin{bmatrix}\n",
        "1      & t_1    & \\cdots & t_1^{n-1} \\\\\n",
        "1      & t_2    & \\cdots & t_2^{n-1} \\\\\n",
        "1      & t_3    & \\cdots & t_3^{n-1} \\\\\n",
        "\\vdots & \\vdots &        & \\vdots    \\\\\n",
        "1      & t_m    & \\cdots & t_m^{n-1} \\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "c_1                                  \\\\\n",
        "c_2                                  \\\\\n",
        "\\vdots                               \\\\\n",
        "c_n\n",
        "\\end{bmatrix}\n",
        "= \\mathbf{V} \\mathbf{c}.\n",
        "$$\n",
        "\n",
        "$\\mathbf{V}$ is $m \\times n$, taller then it is wider.  We cannot solve this exactly.  But Julia can solve it approximately with the same `\\` operator !:\n",
        "\n",
        "```{julia}\n",
        "V = [ t.^0 t ];\n",
        "@show size(V);\n",
        "c = V\\temp;\n",
        "p = Polynomial(c);\n",
        "\n",
        "f = yr -> p((yr-1955)/10);\n",
        "scatter(year,temp,label=\"data\",\n",
        "    xlabel=\"year\",ylabel=\"anomaly (degrees C)\",leg=:bottomright);\n",
        "plot!(f,1955,2000,label=\"linear fit\")\n",
        "\n",
        "```\n",
        "\n",
        "## Least Squares {-}\n",
        "\n",
        "- More generally, linear least-squares problems have the form:\n",
        "\n",
        "$$\n",
        "f(t) = c_1 f_1(t) + \\cdots + c_n f_n(t)\n",
        "$$\n",
        "\n",
        "Where the function $f_i$ are all known functions.\n",
        "\n",
        "- The fit will only be approximate, with *residuals* $y_i - f(t_i)$. \n",
        "\n",
        "- The *least squares* approach minimizes:\n",
        "\n",
        "$$\n",
        "R(c_1,\\ldots,c_n) = \\sum_{i=1}^m\\, [ y_i - f(t_i) ]^2\n",
        "$$\n",
        "- This can be made into a matrix problem: \n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{r} &=\n",
        "\\begin{bmatrix}\n",
        "y_1 \\\\ y_2 \\\\ \\vdots \\\\y_{m-1} \\\\ y_m\n",
        "\\end{bmatrix} -\n",
        "\\begin{bmatrix}\n",
        "f_1(t_1) & f_2(t_1) & \\cdots & f_n(t_1) \\\\[1mm]\n",
        "f_1(t_2) & f_2(t_2) & \\cdots & f_n(t_2) \\\\[1mm]\n",
        "& \\vdots \\\\\n",
        "f_1(t_{m-1}) & f_2(t_{m-1}) & \\cdots & f_n(t_{m-1}) \\\\[1mm]\n",
        "f_1(t_m) & f_2(t_m) & \\cdots & f_n(t_m) \\\\[1mm]\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "c_1 \\\\ c_2 \\\\ \\vdots \\\\ c_n\n",
        "\\end{bmatrix}\\\\\n",
        "&= \\mathbf{b}- \\mathbf{A}\\mathbf{x}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "- The linear least squares problem is then to minimize $R = \\mathbf{r}^T\\mathbf{r}$ or more generally:\n",
        "\n",
        "### Defintion {-} 3.1.3:\n",
        "\n",
        "Given $\\mathbf{A} \\in \\mathscr{R}^{m \\times n}$ and $\\mathbf{b} \\in \\mathscr{R}^m$, with $m > n$, find:\n",
        "\n",
        "$$\n",
        "\\underset{\\mathbf{x} \\in \\mathscr{R}^n }{\\text{argmin}}\\, \\bigl\\| \\mathbf{b}-\\mathbf{A} \\mathbf{x} \\bigr\\|_2^2\n",
        "$$\n",
        "\n",
        "## Change of Variables {-}\n",
        "\n",
        "- Sometimes non-linear fit functions (e.g. $g(t) = a_1 e^{a_2 t}$ ) can be transformed into a linear fit with a change of variables:\n",
        "\n",
        "$$\\log g(t) = \\log a_1 + a_2 t = c_1 + c_2 t$$ \n",
        "- Another example, the power law $y\\approx f(t)=a_1 t^{a_2}$ can be transformed with a log-log transformation into a linear form:\n",
        "\n",
        "$$\n",
        "\\log y \\approx (\\log a_1) + a_2 (\\log t)\n",
        "$$\n",
        "\n",
        "## Exercise 3.1.7 {-}\n",
        "\n",
        "Kepler found that the orbital period $\\tau$ of a planet depends on its mean distance $R$ from the sun according to $\\tau=c R^{\\alpha}$ for a simple rational number $\\alpha$. Perform a linear least-squares fit from the following table in order to determine the most likely simple rational value of $\\alpha$.\n",
        "\n",
        "```{julia}\n",
        "tau = [87.99, 224.7, 365.26, 686.98, 4332.4, 10759, 30684, 60188];\n",
        "R = [57.59, 108.11, 149.57, 227.84, 778.14, 1427, 2870.3, 4499.9];\n",
        "scatter(R,tau,title=\"Orbital Period (days)\", label = \"data\",\n",
        "    xlabel=L\"R (Mkm)\",ylabel=L\"tau\")\n",
        "```\n",
        "\n",
        "Using the log-log transformation:\n",
        "\n",
        "$$\n",
        "\\log \\tau = \\log c + \\alpha \\log R \n",
        "$$\n",
        "\n",
        "```{julia}\n",
        "V = [R.^0 log.(R)];\n",
        "c = V \\ log.(tau);\n",
        "@show c[2]\n",
        "```\n",
        "\n",
        "So the  exponent is close to 3/2, which matches Keplar's third law (usually expressed as $\\tau^2 \\propto R^3$)\n",
        "\n",
        "```{julia}\n",
        "scatter(R,tau,title=\"Orbital Period\", label = \"data\",\n",
        "    xlabel=L\"R\",ylabel=L\"tau\");\n",
        "f = R -> exp(c[1])R^c[2];\n",
        "plot!(f, 50, 5000,label=\"fit\")\n",
        "```\n",
        "\n",
        "## The Normal Equations\n",
        "\n",
        "- Now we want to peal back the curtain and see how to solve the least squares problem. \n",
        "\n",
        "- One solution depends on this Theorem: If $\\mathbf{x}$ satisfies $\\mathbf{A}^T(\\mathbf{A}\\mathbf{x}-\\mathbf{b})=\\boldsymbol{0}$, then $\\mathbf{x}$ solves the linear least-squares problem, i.e., $\\mathbf{x}$ minimizes $\\| \\mathbf{b}-\\mathbf{A}\\mathbf{x} \\|_2$. (Proof in text)\n",
        "\n",
        "- Expanding out  $\\mathbf{A}^T(\\mathbf{A}\\mathbf{x}-\\mathbf{b})=\\boldsymbol{0}$ yields the *normal equations*:\n",
        "\n",
        "$$\n",
        "\\mathbf{A}^T\\mathbf{A}\\mathbf{x}=\\mathbf{A}^T\\mathbf{b}\n",
        "$$\n",
        "\n",
        " \n",
        "\n",
        "## Pseudoinverse and definiteness {-}\n",
        "\n",
        "The normal equations are a square  $n\\times n$ linear system to solve for $\\mathbf{x}$ which leads to the defintion of the *pseudoinverse* as a formal solution:\n",
        "\n",
        "$$\n",
        "\\mathbf{A}^+ = (\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T\n",
        "$$\n",
        "\n",
        "In practice this is not used for the same reason that the ordinary inverse is not used. But conceptually the `\\` operator is mathematically equivalent to left multiplying by the inverse (square matrix) or pseudoinverse (rectangular).\n",
        "\n",
        "The matrix $\\mathbf{A}^T\\mathbf{A}$ has some important properties:\n",
        "\n",
        "1. $\\mathbf{A}^T\\mathbf{A}$ is symmetric\n",
        "\n",
        "2. $\\mathbf{A}^T\\mathbf{A}$ is singular only if the columns of $\\mathbf{A}$  or linearly dependant.\n",
        "\n",
        "3. If $\\mathbf{A}^T\\mathbf{A}$ is nonsingular, that it is positive definate.\n",
        "\n",
        "## Implementation {-}\n",
        "\n",
        "This leads us to a way to solve our linear system, we just use our previous methods to solve the normal equations as a $n\\times n$ system.  Since  $\\mathbf{A}^T\\mathbf{A}$ is symmetric and positive definite, we can use the Cholesky ($\\mathbf{N} = \\mathbf{R}^T\\mathbf{R}$) factorization:\n",
        "\n",
        "```{julia}\n",
        "function lsnormal(A,b)\n",
        "    N = A'*A;  z = A'*b;\n",
        "    R = cholesky(N).U\n",
        "    w = FNC.forwardsub(R',z)                   # solve R'z=c\n",
        "    x = FNC.backsub(R,w)                       # solve Rx=z\n",
        "    return x\n",
        "end\n",
        "```\n",
        "\n",
        "This takes $\\sim (mn^2 + \\frac{1}{3}n^3)$ flops\n",
        "\n",
        "## Conditioning and Stability {-}\n",
        "\n",
        "- The algorithm used by Julia's `\\` does *not* use the normal equations because of instability.  \n",
        "\n",
        "- We need the condition number of a rectangular matrix, which is defined to be:\n",
        "\n",
        "$$\n",
        "\\kappa(\\mathbf{A}) = \\|\\mathbf{A}\\|_2 \\cdot \\|\\mathbf{A}^{+}\\|_2.\n",
        "$$\n",
        "\n",
        "- When the residuals are small, the conditioning of the least squares problem is close to $\\kappa(\\mathbf{A})$. \n",
        "\n",
        "- However, our algorithm uses $\\mathbf{A}^T\\mathbf{A}$ , so the condition number is amplified to $\\kappa(\\mathbf{A}^2)$, which can destabilize the normal equations (increasing the sensitivity to small changes).\n",
        "\n",
        "- Demo:\n",
        "\n",
        "```{julia}\n",
        "t = range(0,3,length=400);\n",
        "f = [ x->sin(x)^2, x->cos((1+1e-7)*x)^2, x->1. ];\n",
        "A = [ f(t) for t in t, f in f ];\n",
        "κ = cond(A)\n",
        "```\n",
        "\n",
        "Set up fake problem with known exact solution (zero residual)\n",
        "\n",
        "```{julia}\n",
        "x = [1.,2,1];\n",
        "b = A*x;\n",
        "```\n",
        "\n",
        "Use backslash:\n",
        "\n",
        "```{julia}\n",
        "x_BS = A\\b;\n",
        "observed_error = norm(x_BS-x)/norm(x);\n",
        "error_bound = κ*eps();\n",
        "@show observed_error\n",
        "@show error_bound\n",
        "```\n",
        "\n",
        "Now try it using normal equations:\n",
        "\n",
        "```{julia}\n",
        "N = A'*A;\n",
        "x_NE = N\\(A'*b);\n",
        "@show observed_err = norm(x_NE-x)/norm(x)\n",
        "@show digits = -log10(observed_err)\n",
        "```\n",
        "\n",
        "*THAT IS ODD* \n",
        "\n",
        "```{julia}\n",
        "x_LSN = lsnormal(A,b);\n",
        "\n",
        "@show observed_err = norm(x_LSN-x)/norm(x)\n",
        "@show digits = -log10(observed_err)\n",
        "```\n",
        "But it does fail with our own implementation.  \n",
        "\n",
        "## Exercise 3.2.4 {-}\n",
        "\n",
        "Prove that if $\\mathbf{A}$ is an invertible square matrix, then $\\mathbf{A}^+=\\mathbf{A}^{-1}$.\n",
        "\n",
        "First we note that if $\\mathbf{A}$ is invertable then so is its transpose:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{A} \\mathbf{A}^{-1} &= I\\\\\n",
        "(\\mathbf{A} \\mathbf{A}^{-1})^T &= I\\\\\n",
        "(\\mathbf{A}^{-1})^T \\mathbf{A}^T &= I\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "So the inverse of $\\mathbf{A}^T$ is $(\\mathbf{A}^T)^{-1} = (\\mathbf{A}^{-1})^T$  (Sometimes written as $\\mathbf{A}^{-T}$)\n",
        "So with that we can use the fact that the inverse of a product of two matrices is the product of the inverses in reverse order to find:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{A}^+ &= (\\mathbf{A}^T\\mathbf{A})^{-1}\\mathbf{A}^T \\\\\n",
        "&=\\mathbf{A}^{-1}(\\mathbf{A^T})^{-1}A^T\\\\\n",
        "&= \\mathbf{A}^{-1}\n",
        "\\end{aligned}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "##  QR factorization\n",
        "\n",
        "### Orthogonal and ONC matrices {-}\n",
        "\n",
        "* *orthogonal* : $\\mathbf{u}^T\\mathbf{v} = 0$ \n",
        "\n",
        "* *orthonormal* : orthogonal +  $\\mathbf{u}^T\\mathbf{u} = 1$\n",
        "\n",
        "* *ONC* : A matrix who's columns are an orthonormal collection. \n",
        "\n",
        "\n",
        "Properties of $n\\times k$ matrix:\n",
        "\n",
        "* $\\mathbf{Q}^T \\mathbf{Q}= I$ ($k\\times k$ identity)\n",
        "\n",
        "* $||\\mathbf{Q}\\mathbf{x}||_2 = ||\\mathbf{x}||_2$\n",
        "\n",
        "* $||\\mathbf{Q}||_2 = 1$\n",
        "\n",
        "* *orthogonal matrix*: A square ONC matrix \n",
        "\n",
        "\n",
        "Suppose $\\mathbf{Q}$ is an $n\\times n$ real orthogonal matrix. Then:\n",
        "1. $\\mathbf{Q}^T = \\mathbf{Q}^{-1}$.\n",
        "2. $\\mathbf{Q}^T$ is also an orthogonal matrix.\n",
        "3. $\\kappa(\\mathbf{Q})=1$ in the 2-norm.\n",
        "4. For any other $n\\times n$ matrix $\\mathbf{A}$, $\\| \\mathbf{A}\\mathbf{Q} \\|_2=\\| \\mathbf{A} \\|_2$.\n",
        "5. If $\\mathbf{U}$ is another $n\\times n$ orthogonal matrix, then $\\mathbf{Q}\\mathbf{U}$ is also orthogonal.\n",
        "\n",
        "\n",
        "## Orthogonal Factorization {-}\n",
        "\n",
        "\n",
        "*Theorem*\n",
        "Every real $m\\times n$ matrix $\\mathbf{A}$ ($m\\ge n$) can be written as $\\mathbf{A}=\\mathbf{Q}\\mathbf{R}$, where $\\mathbf{Q}$ is an $m\\times m$ orthogonal matrix and $\\mathbf{R}$ is an $m\\times n$ upper triangular matrix.\n",
        "\n",
        "\n",
        "\n",
        "* Thin QR:  $\\mathbf{A} = \\hat{\\mathbf{Q}} \\hat{\\mathbf{R}}$, where $\\hat{\\mathbf{Q}}$ is $m\\times n$ and ONC, and $\\hat{\\mathbf{R}}$ is $n\\times n$ and upper triangular.\n",
        "\n",
        "```{julia}\n",
        "A = rand(1.:9.,6,4);\n",
        "Q,R = qr(A);\n",
        "Q\n",
        "```\n",
        "We can also get the thin from Q by converting to matrix:\n",
        "\n",
        "```{julia}\n",
        "Q_hat = Matrix(Q)\n",
        "```\n",
        "```{julia}\n",
        "Q_hat'*Q_hat\n",
        "```\n",
        "\n",
        "## Least squares and QR {-}\n",
        "\n",
        "We can us this to find the least square solution to  $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$ by substituting the QR factorization of $\\mathbf{A}$ into the normal equations:\n",
        "\n",
        "$$\n",
        "\\begin{split}\n",
        "  \\mathbf{A}^T\\mathbf{A} \\mathbf{x} &= \\mathbf{A}^T \\mathbf{b}, \\\\\n",
        "  \\hat{\\mathbf{R}}^T \\hat{\\mathbf{Q}}^T \\hat{\\mathbf{Q}} \\hat{\\mathbf{R}} \\mathbf{x} &= \\hat{\\mathbf{R}}^T \\hat{\\mathbf{Q}}^T \\mathbf{b}, \\\\\n",
        "  \\hat{\\mathbf{R}}^T \\hat{\\mathbf{R}} \\mathbf{x}& = \\hat{\\mathbf{R}}^T \\hat{\\mathbf{Q}}^T \\mathbf{b}.\n",
        "\\end{split}\n",
        "$$\n",
        "\n",
        "As long as $\\mathbf{A}$ is not rank deficient, we then have $\\hat{\\mathbf{R}} \\mathbf{x}=\\hat{\\mathbf{Q}}^T \\mathbf{b}$. Since $R$ is upper triangular we can solve this using back subsitution!\n",
        "\n",
        "```{julia}\n",
        "function lsqrfact(A,b)\n",
        "    Q,R = qr(A)\n",
        "    c = Q'*b\n",
        "    x = FNC.backsub(R,c)\n",
        "    return x\n",
        "end\n",
        "```\n",
        "\n",
        "Does this improve our previous solution to demo in 3.2?\n",
        "\n",
        "```{julia}\n",
        "t = range(0,3,length=400);\n",
        "f = [ x->sin(x)^2, x->cos((1+1e-7)*x)^2, x->1. ];\n",
        "A = [ f(t) for t in t, f in f ];\n",
        " \n",
        "x = [1.,2,1];\n",
        "b = A*x;\n",
        "observed_error = norm(lsqrfact(A,b)-x)/norm(x);\n",
        "@show observed_error\n",
        "κ = cond(A);\n",
        "@show error_bound = κ*eps()\n",
        "```\n",
        "\n",
        "\n",
        "## Exercise 3.3.7 {-}\n",
        "\n",
        "Repeat Exercise 3.1.2 but use thin QR factorization rather than backlash operator\n",
        "\n",
        "```{julia}\n",
        "counts = [3.929, 5.308, 7.240, 9.638, 12.87, 17.07, 23.19, 31.44, 39.82, 50.19, 62.95, 76.21, 92.22, 106.0, 122.8, 132.2, 150.7, 179.3, 203.3, 226.5, 248.7, 281.4, 308.7];\n",
        "\n",
        "years =  collect(1790:10:2010);\n",
        "scatter(years, counts,title=\"US Population)\", label = \"data\",\n",
        "    xlabel=L\"Year\",ylabel=L\"millions\")\n",
        "\n",
        "```\n",
        "\n",
        "```{julia}\n",
        "A = [years.^0 years.^1 years.^2 years.^3];\n",
        "c = FNC.lsqrfact(A, counts)\n",
        "  \n",
        "```\n",
        "```{julia}\n",
        "p = Polynomial(c);\n",
        "plot!(p,1790,2010,label=\"interpolant\")\n",
        "\n",
        "```\n",
        "\n",
        "Predicted population in 2020:\n",
        "\n",
        "```{julia}\n",
        "p(2020)\n",
        "```\n",
        "Actual: 331.4\n",
        "\n",
        "\n",
        "\n",
        "## Computing QR factorizations\n",
        "\n",
        "QR factorization can be computed with Gram-Schmidt process. This section of the book shows how this is down mechanically using *Householder reflections*. \n",
        "\n",
        "### Householder reflections {-}\n",
        "\n",
        "* Householder reflector is a matrix of the form: $\\mathbf{P} = \\mathbf{I} - 2 \\mathbf{v}\\mathbf{v}^T$ where $v$ is a unit vector. \n",
        "\n",
        "* Note that $\\mathbf{P}$ is orthogonal and for any vector $\\mathbf{x}$: \n",
        "\n",
        "$$\n",
        "\\mathbf{P}\\mathbf{x} = \\mathbf{x} - 2 \\mathbf{v} (\\mathbf{v}^T\\mathbf{x})\n",
        "$$\n",
        "\n",
        "This is a *reflection* of $\\mathbf{x}$ about the hyperplane with normal vector $\\mathbf{v}$\n",
        "\n",
        "\n",
        "## Factorization Algorithm {-}\n",
        "\n",
        "How does this help us do the factorization?  The key observation is that given a vector $\\mathbf{z}$ we can choose a $\\mathbf{V}$ so that $\\mathbf{P}$ reflect $\\mathbf{z}$ *onto* the $\\mathbf{e}_1$ axis:\n",
        "\n",
        "$$\n",
        "\\mathbf{P}\\mathbf{z} =\n",
        "\\begin{bmatrix}\n",
        "\\pm \\| \\mathbf{z} \\|\\\\0 \\\\ \\vdots \\\\ 0\n",
        "\\end{bmatrix} = \\pm \\| \\mathbf{z} \\| \\mathbf{e}_1.\n",
        "$$\n",
        "\n",
        "This uses the fact that $\\mathbf{P}$ is orthogonal and so preserves the norm.\n",
        "\n",
        "The vector that will do this is:\n",
        "\n",
        "$$\n",
        "\\mathbf{v} = \\frac{\\mathbf{w}}{||\\mathbf{w}||}\\text{,  }\\mathbf{w} = ||\\mathbf{z}||e_1-z\n",
        "$$\n",
        "\n",
        "\n",
        "The book describes the process in detail, but the essence of the idea is to use this idea to successively turn the matrix $\\mathbf{A}$ into $\\mathbf{R}$.  The orthogonal projection matrices form $\\mathbf{Q}$\n",
        "\n",
        "\n",
        "\n",
        "### Q-less QR and least squares {-}\n",
        "\n",
        "* Since we only need $\\mathbf{Q}$  to compute $\\mathbf{Q}^T b$, we don't need the full $\\mathbf{Q}$\n",
        "\n",
        "* Leads to \"Q-less\" factorization: In *julia* a special \"QRCompactWYQ\" object is returned for efficient calculation of $\\mathbf{Q}^T b$ \n",
        "\n",
        "## Exercise 3.4.1 {-}\n",
        "\n",
        "Find a Householder reflector $\\mathbf{P}$ such that\n",
        "  \n",
        "$$\n",
        "    \\mathbf{P}\n",
        "    \\begin{bmatrix}\n",
        "      2 \\\\ 9 \\\\ -6\n",
        "    \\end{bmatrix} =\n",
        "    \\begin{bmatrix}\n",
        "      11\\\\0\\\\0\n",
        "    \\end{bmatrix}.\n",
        "$$\n",
        "\n",
        "\n",
        "```{julia}\n",
        "\n",
        "z = [2,9,-6]; \n",
        "\n",
        "FNC.norm(z)\n",
        "```\n",
        "\n",
        "This is of the form that we can apply Theorem 3.4.2\n",
        "\n",
        "```{julia}\n",
        "e1 = [1,0,0];\n",
        "\n",
        "w = FNC.norm(z)*e1 - z;\n",
        "v = w/FNC.norm(w);\n",
        "\n",
        "P =  I - 2*v*v';\n",
        "\n",
        "# verify\n",
        "P*z\n",
        "\n",
        "```\n",
        "\n",
        " "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
